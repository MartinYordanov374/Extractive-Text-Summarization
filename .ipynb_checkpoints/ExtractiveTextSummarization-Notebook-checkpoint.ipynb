{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "384da850",
   "metadata": {},
   "source": [
    "<h1>Extractive text summarization in python</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb607d8",
   "metadata": {},
   "source": [
    "<p>Extractive text summarization is a \"technique\" for summarizing text data. What's characteristic for this kind of summarization is that the summary it produces is a carbon copy of the most important sentences given in the text to be summarized.</p>\n",
    "\n",
    "<p>The way I achieve this here is through using term frequency - inverse document frequency(TF-IDF) over the entire text body. In short, the text input data is taken, it is split into different sentences, and TF-IDF calculation os executed over each sentence. All the TF-IDF scores are, then, saved in a list and sorted from highest to lowesr, thus the resulting sentences are ordered from most to least relevant.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ecb6b2",
   "metadata": {},
   "source": [
    "<h1>Setting the project up</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df04543",
   "metadata": {},
   "source": [
    "<p>Before getting to the fun part, I need to import a couple of libraries. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "163c6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import contractions\n",
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f59d5",
   "metadata": {},
   "source": [
    "<p> Before pre-processing any sort of data I need to have data, of course. I've taken a fragment of an article about the black thursday from investopedia. You can check it out <a href='https://www.investopedia.com/terms/b/blackthursday.asp'> here. </a>  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23145dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "InputText = \"\"\"Black Thursday marked the beginning of the end of one of the longest-running bull markets in U.S. history. For nearly the entire decade of the 1920s, stock prices had been steadily climbing, rising to unprecedented heights. The Dow Jones Industrial Average (DJIA) increased sixfold from 63 in August 1921 to 381 in September 1929.\n",
    "However, even before the New York Stock Exchange (NYSE) opened on that fateful Thursday in 1929, the elevated equity prices were making investors and financial experts uneasy. On Sept. 5, at the annual National Business Conference, economist Roger Babson predicted that “sooner or later a crash is coming, and it may be terrific.” Throughout September, stock prices gyrated, with sudden declines and rapid recoveries.\n",
    "The jitters continued into October. In fact, on Oct. 23, the Dow fell 4.6%. A Washington Post headline exclaimed, “Huge Selling Wave Creates Near-Panic as Stocks Collapse.”\n",
    "By this time, the stock market had already fallen nearly 20% since its record close of 381 on Sept. 3. When trading opened on Thursday, Oct. 24, the Dow fell 11% in the first few hours.  Even more ominous was the heavy trading volume: It was to hit a record 12.9 million shares—three times the normal amount—by day’s end.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "618e75f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Black Thursday marked the beginning of the end of one of the longest-running bull markets in U.S. history. For nearly the entire decade of the 1920s, stock prices had been steadily climbing, rising to unprecedented heights. The Dow Jones Industrial Average (DJIA) increased sixfold from 63 in August 1921 to 381 in September 1929.\\nHowever, even before the New York Stock Exchange (NYSE) opened on that fateful Thursday in 1929, the elevated equity prices were making investors and financial experts uneasy. On Sept. 5, at the annual National Business Conference, economist Roger Babson predicted that “sooner or later a crash is coming, and it may be terrific.” Throughout September, stock prices gyrated, with sudden declines and rapid recoveries.\\nThe jitters continued into October. In fact, on Oct. 23, the Dow fell 4.6%. A Washington Post headline exclaimed, “Huge Selling Wave Creates Near-Panic as Stocks Collapse.”\\nBy this time, the stock market had already fallen nearly 20% since its record close of 381 on Sept. 3. When trading opened on Thursday, Oct. 24, the Dow fell 11% in the first few hours.  Even more ominous was the heavy trading volume: It was to hit a record 12.9 million shares—three times the normal amount—by day’s end.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e734b0",
   "metadata": {},
   "source": [
    "<p>It's a fairly short text, but it is enough for the purposes of this notebook. After all we're not training a model to recognize specific texts, we don't need much data for what we're trying to do. One sample text like that is enough, of course - the longer the text, the better the result would be (I suppose).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b0a80e",
   "metadata": {},
   "source": [
    "<h1>Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f19aa83",
   "metadata": {},
   "source": [
    "<p>What we now need to do is to split the text into different sentences (every sentence ends with a period or a semicolon), save them in a list and remove the so-called stop words from each sentence. Stop words are the words that do not carry much meaning by themselves, if any. In English, those words are \"the\", \"and\", \"but\" etc.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "805aeb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "InputTextSentences = sent_tokenize(InputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e39b06ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Black Thursday marked the beginning of the end of one of the longest-running bull markets in U.S. history.',\n",
       " 'For nearly the entire decade of the 1920s, stock prices had been steadily climbing, rising to unprecedented heights.',\n",
       " 'The Dow Jones Industrial Average (DJIA) increased sixfold from 63 in August 1921 to 381 in September 1929.',\n",
       " 'However, even before the New York Stock Exchange (NYSE) opened on that fateful Thursday in 1929, the elevated equity prices were making investors and financial experts uneasy.',\n",
       " 'On Sept. 5, at the annual National Business Conference, economist Roger Babson predicted that “sooner or later a crash is coming, and it may be terrific.” Throughout September, stock prices gyrated, with sudden declines and rapid recoveries.',\n",
       " 'The jitters continued into October.',\n",
       " 'In fact, on Oct. 23, the Dow fell 4.6%.',\n",
       " 'A Washington Post headline exclaimed, “Huge Selling Wave Creates Near-Panic as Stocks Collapse.”\\nBy this time, the stock market had already fallen nearly 20% since its record close of 381 on Sept. 3.',\n",
       " 'When trading opened on Thursday, Oct. 24, the Dow fell 11% in the first few hours.',\n",
       " 'Even more ominous was the heavy trading volume: It was to hit a record 12.9 million shares—three times the normal amount—by day’s end.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputTextSentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd60da",
   "metadata": {},
   "source": [
    "<p>Now that we have all the sentences in the text, we can tokenize each word (split the sentence into words) just like we did with the sentences in the previous cells. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5802e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "WordsInSentenceList = [word_tokenize(sentence.lower()) for sentence in InputTextSentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e7f98ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['black',\n",
       "  'thursday',\n",
       "  'marked',\n",
       "  'the',\n",
       "  'beginning',\n",
       "  'of',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'longest-running',\n",
       "  'bull',\n",
       "  'markets',\n",
       "  'in',\n",
       "  'u.s.',\n",
       "  'history',\n",
       "  '.'],\n",
       " ['for',\n",
       "  'nearly',\n",
       "  'the',\n",
       "  'entire',\n",
       "  'decade',\n",
       "  'of',\n",
       "  'the',\n",
       "  '1920s',\n",
       "  ',',\n",
       "  'stock',\n",
       "  'prices',\n",
       "  'had',\n",
       "  'been',\n",
       "  'steadily',\n",
       "  'climbing',\n",
       "  ',',\n",
       "  'rising',\n",
       "  'to',\n",
       "  'unprecedented',\n",
       "  'heights',\n",
       "  '.']]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordsInSentenceList[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e422d2",
   "metadata": {},
   "source": [
    "<p>Next, we need to remove the stopwords from the text. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "32a3b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoStopWordsSentences = [word for sentence in WordsInSentenceList for word in sentence if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3fe1bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoStopWordsSentencesJoined = \" \".join(NoStopWordsSentences).split(' . ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1460ea36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nearly entire decade 1920s , stock prices steadily climbing , rising unprecedented heights'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoStopWordsSentencesJoined[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a658e7",
   "metadata": {},
   "source": [
    "<p>Let's remove any punctuation now, so that we don't get any unpleasant surprises down in the notebook.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5d3c3b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "RegexTokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ea015c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoPunctuationSentences = [RegexTokenizer.tokenize(sentence) for sentence in NoStopWordsSentencesJoined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "39d75aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['black',\n",
       "  'thursday',\n",
       "  'marked',\n",
       "  'beginning',\n",
       "  'end',\n",
       "  'one',\n",
       "  'longest',\n",
       "  'running',\n",
       "  'bull',\n",
       "  'markets',\n",
       "  'u',\n",
       "  's',\n",
       "  'history'],\n",
       " ['nearly',\n",
       "  'entire',\n",
       "  'decade',\n",
       "  '1920s',\n",
       "  'stock',\n",
       "  'prices',\n",
       "  'steadily',\n",
       "  'climbing',\n",
       "  'rising',\n",
       "  'unprecedented',\n",
       "  'heights']]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoPunctuationSentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "70a267f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcessedData = [\" \".join(sentence) for sentence in NoPunctuationSentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4050c2f9",
   "metadata": {},
   "source": [
    "<p>Our data seems to be clean now, ready to be processed by the algorithm we will use ! </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e564b6",
   "metadata": {},
   "source": [
    "<h1>Data processing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8278eadf",
   "metadata": {},
   "source": [
    "<p>The first thing we've got to do is to represent our data as numbers, so that the computer can understand it. This we will achieve through the TfIdfVectorizer, thanks to scikit learn :) </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6353fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1e98d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "VectorizedDataMatrix = Vectorizer.fit_transform(ProcessedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d195e0d",
   "metadata": {},
   "source": [
    "<p>What we just did on the cell above, is - take the input data, turn it into numbers, return the tf-idf value for each word and save it all to a matrix, hence the variable name. The tf-idf value is the key thing here, this is the formula through which the tf-idf value is calculated for every word in our text:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f758d",
   "metadata": {},
   "source": [
    "$TFIDF(S) = TF(S) * IDF(S)$, where TF is the frequency of a given term - S - in a given text and idf is the inverse-document frequency of the given term.: $IDF = log( N / df)$, where N is the total number of documents/texts/sentences(depends on the case) that contain the given term S and df is the number of all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f255a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

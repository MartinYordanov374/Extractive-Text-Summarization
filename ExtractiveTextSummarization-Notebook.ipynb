{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba57c0d",
   "metadata": {},
   "source": [
    "<h1>Extractive text summarization in python</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f024a",
   "metadata": {},
   "source": [
    "<p>Extractive text summarization is a \"technique\" for summarizing text data. What's characteristic for this kind of summarization is that the summary it produces is a carbon copy of the most important sentences given in the text to be summarized.</p>\n",
    "\n",
    "<p>The way I achieve this here is through using term frequency - inverse document frequency(TF-IDF) over the entire text body. In short, the text input data is taken, it is split into different sentences, and TF-IDF calculation os executed over each sentence. All the TF-IDF scores are, then, saved in a list and sorted from highest to lowesr, thus the resulting sentences are ordered from most to least relevant.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95857181",
   "metadata": {},
   "source": [
    "<h1>Setting the project up</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef14aa",
   "metadata": {},
   "source": [
    "<p>Before getting to the fun part, I need to import a couple of libraries. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea533c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ada469",
   "metadata": {},
   "source": [
    "<p> Before pre-processing any sort of data I need to have data, of course. I've taken a fragment of an article about the black thursday from investopedia. You can check it out <a>  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e60315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "InputText = \"\"\"Black Thursday marked the beginning of the end of one of the longest-running bull markets in U.S. history. For nearly the entire decade of the 1920s, stock prices had been steadily climbing, rising to unprecedented heights. The Dow Jones Industrial Average (DJIA) increased sixfold from 63 in August 1921 to 381 in September 1929.\n",
    "However, even before the New York Stock Exchange (NYSE) opened on that fateful Thursday in 1929, the elevated equity prices were making investors and financial experts uneasy. On Sept. 5, at the annual National Business Conference, economist Roger Babson predicted that “sooner or later a crash is coming, and it may be terrific.” Throughout September, stock prices gyrated, with sudden declines and rapid recoveries.\n",
    "The jitters continued into October. In fact, on Oct. 23, the Dow fell 4.6%. A Washington Post headline exclaimed, “Huge Selling Wave Creates Near-Panic as Stocks Collapse.”\n",
    "By this time, the stock market had already fallen nearly 20% since its record close of 381 on Sept. 3. When trading opened on Thursday, Oct. 24, the Dow fell 11% in the first few hours.  Even more ominous was the heavy trading volume: It was to hit a record 12.9 million shares—three times the normal amount—by day’s end.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ddc5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

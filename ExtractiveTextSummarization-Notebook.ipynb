{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "384da850",
   "metadata": {},
   "source": [
    "<h1>Extractive text summarization in python</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb607d8",
   "metadata": {},
   "source": [
    "<p>Extractive text summarization is a \"technique\" for summarizing text data. What's characteristic for this kind of summarization is that the summary it produces is a carbon copy of the most important sentences given in the text to be summarized.</p>\n",
    "\n",
    "<p>The way I achieve this here is through using term frequency - inverse document frequency(TF-IDF) over the entire text body. In short, the text input data is taken, it is split into different sentences, and TF-IDF calculation os executed over each sentence. All the TF-IDF scores are, then, saved in a list and sorted from highest to lowesr, thus the resulting sentences are ordered from most to least relevant.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ecb6b2",
   "metadata": {},
   "source": [
    "<h1>Setting the project up</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df04543",
   "metadata": {},
   "source": [
    "<p>Before getting to the fun part, I need to import a couple of libraries. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "163c6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import contractions\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f59d5",
   "metadata": {},
   "source": [
    "<p> Before pre-processing any sort of data I need to have data, of course. I've taken a fragment of an article about the black thursday from investopedia. You can check it out <a href='https://www.investopedia.com/terms/b/blackthursday.asp'> here. </a>  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "23145dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "InputText = \"\"\"Black Thursday marked the beginning of the end of one of the longest-running bull markets in U.S. history. For nearly the entire decade of the 1920s, stock prices had been steadily climbing, rising to unprecedented heights. The Dow Jones Industrial Average (DJIA) increased sixfold from 63 in August 1921 to 381 in September 1929.\n",
    "However, even before the New York Stock Exchange (NYSE) opened on that fateful Thursday in 1929, the elevated equity prices were making investors and financial experts uneasy. On Sept. 5, at the annual National Business Conference, economist Roger Babson predicted that “sooner or later a crash is coming, and it may be terrific.” Throughout September, stock prices gyrated, with sudden declines and rapid recoveries.\n",
    "The jitters continued into October. In fact, on Oct. 23, the Dow fell 4.6%. A Washington Post headline exclaimed, “Huge Selling Wave Creates Near-Panic as Stocks Collapse.”\n",
    "By this time, the stock market had already fallen nearly 20% since its record close of 381 on Sept. 3. When trading opened on Thursday, Oct. 24, the Dow fell 11% in the first few hours.  Even more ominous was the heavy trading volume: It was to hit a record 12.9 million shares—three times the normal amount—by day’s end.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "618e75f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Black Thursday marked the beginning of the end of one of the longest-running bull markets in U.S. history. For nearly the entire decade of the 1920s, stock prices had been steadily climbing, rising to unprecedented heights. The Dow Jones Industrial Average (DJIA) increased sixfold from 63 in August 1921 to 381 in September 1929.\\nHowever, even before the New York Stock Exchange (NYSE) opened on that fateful Thursday in 1929, the elevated equity prices were making investors and financial experts uneasy. On Sept. 5, at the annual National Business Conference, economist Roger Babson predicted that “sooner or later a crash is coming, and it may be terrific.” Throughout September, stock prices gyrated, with sudden declines and rapid recoveries.\\nThe jitters continued into October. In fact, on Oct. 23, the Dow fell 4.6%. A Washington Post headline exclaimed, “Huge Selling Wave Creates Near-Panic as Stocks Collapse.”\\nBy this time, the stock market had already fallen nearly 20% since its record close of 381 on Sept. 3. When trading opened on Thursday, Oct. 24, the Dow fell 11% in the first few hours.  Even more ominous was the heavy trading volume: It was to hit a record 12.9 million shares—three times the normal amount—by day’s end.'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e734b0",
   "metadata": {},
   "source": [
    "<p>It's a fairly short text, but it is enough for the purposes of this notebook. After all we're not training a model to recognize specific texts, we don't need much data for what we're trying to do. One sample text like that is enough, of course - the longer the text, the better the result would be (I suppose).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b0a80e",
   "metadata": {},
   "source": [
    "<h1>Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f19aa83",
   "metadata": {},
   "source": [
    "<p>What we now need to do is to split the text into different sentences (every sentence ends with a period or a semicolon), save them in a list and remove the so-called stop words from each sentence. Stop words are the words that do not carry much meaning by themselves, if any. In English, those words are \"the\", \"and\", \"but\" etc.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "805aeb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "InputTextSentences = sent_tokenize(InputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e39b06ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Black Thursday marked the beginning of the end of one of the longest-running bull markets in U.S. history.',\n",
       " 'For nearly the entire decade of the 1920s, stock prices had been steadily climbing, rising to unprecedented heights.',\n",
       " 'The Dow Jones Industrial Average (DJIA) increased sixfold from 63 in August 1921 to 381 in September 1929.',\n",
       " 'However, even before the New York Stock Exchange (NYSE) opened on that fateful Thursday in 1929, the elevated equity prices were making investors and financial experts uneasy.',\n",
       " 'On Sept. 5, at the annual National Business Conference, economist Roger Babson predicted that “sooner or later a crash is coming, and it may be terrific.” Throughout September, stock prices gyrated, with sudden declines and rapid recoveries.',\n",
       " 'The jitters continued into October.',\n",
       " 'In fact, on Oct. 23, the Dow fell 4.6%.',\n",
       " 'A Washington Post headline exclaimed, “Huge Selling Wave Creates Near-Panic as Stocks Collapse.”\\nBy this time, the stock market had already fallen nearly 20% since its record close of 381 on Sept. 3.',\n",
       " 'When trading opened on Thursday, Oct. 24, the Dow fell 11% in the first few hours.',\n",
       " 'Even more ominous was the heavy trading volume: It was to hit a record 12.9 million shares—three times the normal amount—by day’s end.']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputTextSentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd60da",
   "metadata": {},
   "source": [
    "<p>Now that we have all the sentences in the text, we can tokenize each word (split the sentence into words) just like we did with the sentences in the previous cells. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5802e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "WordsInSentenceList = [word_tokenize(sentence.lower()) for sentence in InputTextSentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e7f98ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['black',\n",
       "  'thursday',\n",
       "  'marked',\n",
       "  'the',\n",
       "  'beginning',\n",
       "  'of',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'longest-running',\n",
       "  'bull',\n",
       "  'markets',\n",
       "  'in',\n",
       "  'u.s.',\n",
       "  'history',\n",
       "  '.'],\n",
       " ['for',\n",
       "  'nearly',\n",
       "  'the',\n",
       "  'entire',\n",
       "  'decade',\n",
       "  'of',\n",
       "  'the',\n",
       "  '1920s',\n",
       "  ',',\n",
       "  'stock',\n",
       "  'prices',\n",
       "  'had',\n",
       "  'been',\n",
       "  'steadily',\n",
       "  'climbing',\n",
       "  ',',\n",
       "  'rising',\n",
       "  'to',\n",
       "  'unprecedented',\n",
       "  'heights',\n",
       "  '.']]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordsInSentenceList[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e422d2",
   "metadata": {},
   "source": [
    "<p>Next, we need to remove the stopwords from the text. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "32a3b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoStopWordsSentences = [word for sentence in WordsInSentenceList for word in sentence if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3fe1bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoStopWordsSentencesJoined = \" \".join(NoStopWordsSentences).split(' . ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1460ea36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nearly entire decade 1920s , stock prices steadily climbing , rising unprecedented heights'"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoStopWordsSentencesJoined[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a658e7",
   "metadata": {},
   "source": [
    "<p>Let's remove any punctuation now, so that we don't get any unpleasant surprises down in the notebook.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5d3c3b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "RegexTokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ea015c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoPunctuationSentences = [RegexTokenizer.tokenize(sentence) for sentence in NoStopWordsSentencesJoined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "39d75aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['black',\n",
       "  'thursday',\n",
       "  'marked',\n",
       "  'beginning',\n",
       "  'end',\n",
       "  'one',\n",
       "  'longest',\n",
       "  'running',\n",
       "  'bull',\n",
       "  'markets',\n",
       "  'u',\n",
       "  's',\n",
       "  'history'],\n",
       " ['nearly',\n",
       "  'entire',\n",
       "  'decade',\n",
       "  '1920s',\n",
       "  'stock',\n",
       "  'prices',\n",
       "  'steadily',\n",
       "  'climbing',\n",
       "  'rising',\n",
       "  'unprecedented',\n",
       "  'heights']]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoPunctuationSentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "70a267f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcessedData = [\" \".join(sentence) for sentence in NoPunctuationSentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4050c2f9",
   "metadata": {},
   "source": [
    "<p>Our data seems to be clean now, ready to be processed by the algorithm we will use ! </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e564b6",
   "metadata": {},
   "source": [
    "<h1>Data processing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8278eadf",
   "metadata": {},
   "source": [
    "<p>The first thing we've got to do is to represent our data as numbers, so that the computer can understand it. This we will achieve through the TfIdfVectorizer, thanks to scikit learn :) </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6353fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1e98d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "VectorizedDataMatrix = Vectorizer.fit_transform(ProcessedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d195e0d",
   "metadata": {},
   "source": [
    "<p>What we just did on the cell above, is - take the input data, turn it into numbers, return the tf-idf value for each word and save it all to a matrix, hence the variable name. The tf-idf value is the key thing here, this is the formula through which the tf-idf value is calculated for every word in our text:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f758d",
   "metadata": {},
   "source": [
    "$TF-IDF(S) = TF(S) * IDF(S)$, where TF is the frequency of a given term - S - in a given text and idf is the inverse-document frequency of the given term.: $IDF = log( N / df)$, where N is the total number of documents/texts/sentences(depends on the case) that contain the given term S and df is the number of all the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc16d358",
   "metadata": {},
   "source": [
    "<p>Here we create a similarity matrix, by using cosine similarity. We will use it later on to find out how similar each sentence is to the rest of the sentences - because the more similar sentences are to each other (out of the whole text) the more likely it is that they are very important to the given text (this is after we've transformed the data using tf-idf). </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3d1f255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarityMatrix = cosine_similarity(VectorizedDataMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f56f9737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.04220999, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.06101855, 0.05655268],\n",
       "       [0.        , 1.        , 0.        , 0.0777334 , 0.06708808,\n",
       "        0.        , 0.        , 0.07876119, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        , 0.05163449, 0.04456332,\n",
       "        0.        , 0.08079794, 0.04467079, 0.05713277, 0.        ],\n",
       "       [0.04220999, 0.0777334 , 0.05163449, 1.        , 0.05114641,\n",
       "        0.        , 0.        , 0.02263461, 0.11035986, 0.04434591],\n",
       "       [0.        , 0.06708808, 0.04456332, 0.05114641, 1.        ,\n",
       "        0.        , 0.        , 0.05182266, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.08079794, 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.35341582, 0.        ],\n",
       "       [0.        , 0.07876119, 0.04467079, 0.02263461, 0.05182266,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.03836519],\n",
       "       [0.06101855, 0.        , 0.05713277, 0.11035986, 0.        ,\n",
       "        0.        , 0.35341582, 0.        , 1.        , 0.06410623],\n",
       "       [0.05655268, 0.        , 0.        , 0.04434591, 0.        ,\n",
       "        0.        , 0.        , 0.03836519, 0.06410623, 1.        ]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarityMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec5c5e",
   "metadata": {},
   "source": [
    "<p>Here we calculate the cumulative similarity of each cosine similarity tow in the cosine similarity matrix, and we find its average by dividing it over the amount of rows the cosine similarity matrix has. In addition to the average similarity, we also save the similarity ID (the index of the sentence, in the input text, the similarity has been calculated for) in an object which is saved in a list that holds all the similarities.  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c285e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = []\n",
    "for index, similarity in enumerate(similarityMatrix):\n",
    "    similarities.append({'similarityID': index, 'similarity': np.sum(similarity[1]) / len(similarityMatrix)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb34fb",
   "metadata": {},
   "source": [
    "<p>We've got to sort the similarity objects by the cosine similarity value.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "cf709892",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedSimilarities = sorted(similarities, key = lambda x: x['similarity'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "88e6dea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'similarityID': 1, 'similarity': 0.1},\n",
       " {'similarityID': 7, 'similarity': 0.007876118798456924},\n",
       " {'similarityID': 3, 'similarity': 0.007773340465636438},\n",
       " {'similarityID': 4, 'similarity': 0.006708807688799527},\n",
       " {'similarityID': 0, 'similarity': 0.0},\n",
       " {'similarityID': 2, 'similarity': 0.0},\n",
       " {'similarityID': 5, 'similarity': 0.0},\n",
       " {'similarityID': 6, 'similarity': 0.0},\n",
       " {'similarityID': 8, 'similarity': 0.0},\n",
       " {'similarityID': 9, 'similarity': 0.0}]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedSimilarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f82e0b",
   "metadata": {},
   "source": [
    "<p>What we've got left to do is take the half of the input text's length and round it to the nearest, lower, number.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "fe5b57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SummaryLength = math.floor(len(InputTextSentences) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "27c05e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "for sentenceID in range(SummaryLength):\n",
    "    targetSentenceID = sortedSimilarities[sentenceID]['similarityID']\n",
    "    summary.append(InputTextSentences[targetSentenceID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "099901cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For nearly the entire decade of the 1920s, stock prices had been steadily climbing, rising to unprecedented heights. A Washington Post headline exclaimed, “Huge Selling Wave Creates Near-Panic as Stocks Collapse.”\\nBy this time, the stock market had already fallen nearly 20% since its record close of 381 on Sept. 3. However, even before the New York Stock Exchange (NYSE) opened on that fateful Thursday in 1929, the elevated equity prices were making investors and financial experts uneasy. On Sept. 5, at the annual National Business Conference, economist Roger Babson predicted that “sooner or later a crash is coming, and it may be terrific.” Throughout September, stock prices gyrated, with sudden declines and rapid recoveries. Black Thursday marked the beginning of the end of one of the longest-running bull markets in U.S. history.'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a41daf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
